#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–≤–µ—Ä–µ–π –õ–∞–±–∏—Ä–∏–Ω—Ç
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Å–µ—Ö –¥–≤–µ—Ä—è—Ö —Å —Å–∞–π—Ç–∞ labirintdoors.ru
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from datetime import datetime
from typing import List, Dict, Optional
from fake_useragent import UserAgent
from tqdm import tqdm
import pandas as pd


class LabirintParser:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞ –¥–≤–µ—Ä–µ–π –õ–∞–±–∏—Ä–∏–Ω—Ç"""
    
    def __init__(self):
        self.base_url = "https://labirintdoors.ru"
        self.catalog_url = f"{self.base_url}/katalog2"
        self.session = requests.Session()
        self.ua = UserAgent()
        self.headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        self.doors_data = []
        
    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å retry –ª–æ–≥–∏–∫–æ–π"""
        for attempt in range(retries):
            try:
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    timeout=30
                )
                response.raise_for_status()
                return BeautifulSoup(response.content, 'lxml')
            except Exception as e:
                print(f"‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retries} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}")
                    return None
    
    def extract_price(self, text: str) -> Optional[int]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        match = re.search(r'(\d+[\s\d]*)\s*—Ä—É–±', text.replace(' ', ''))
        if match:
            return int(match.group(1).replace(' ', ''))
        return None
    
    def parse_catalog_page(self, soup: BeautifulSoup) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã"""
        doors = []
        
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∞—Å—Å—É
        collections = soup.find_all('a', class_='product-sections-01-item')
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {len(collections)}")
        
        for item in tqdm(collections, desc="–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤"):
            try:
                door_data = {}
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (—Å–∞–º —ç–ª–µ–º–µ–Ω—Ç - —ç—Ç–æ —Å—Å—ã–ª–∫–∞)
                href = item.get('href', '')
                if href:
                    door_data['url'] = href if href.startswith('http') else f"{self.base_url}{href}"
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Å—ã–ª–∫–∏ –∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤–Ω—É—Ç—Ä–∏
                text = item.get_text(strip=True)
                if text and '–í—Ö–æ–¥–Ω—ã–µ –¥–≤–µ—Ä–∏' in text:
                    door_data['name'] = text
                elif text:
                    door_data['name'] = f"–í—Ö–æ–¥–Ω—ã–µ –¥–≤–µ—Ä–∏ {text}"
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
                price_match = re.search(r'–æ—Ç\s+(\d+[\s\d]*)\s*—Ä—É–±', text)
                if price_match:
                    price_str = price_match.group(1).replace(' ', '')
                    door_data['price'] = int(price_str)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img = item.find('img')
                if img:
                    img_src = img.get('src') or img.get('data-src') or img.get('data-lazy')
                    if img_src:
                        door_data['image'] = img_src if img_src.startswith('http') else f"{self.base_url}{img_src}"
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç—É
                if 'LEOLAB' in text or 'LEO' in text:
                    door_data['category'] = '–ù–æ–≤–∏–Ω–∫–∏ 2025'
                elif 'PIANO' in text or 'ROYAL' in text or 'ISSIDA' in text:
                    door_data['category'] = '–•–∏—Ç—ã –ø—Ä–æ–¥–∞–∂'
                elif '—Ç–µ—Ä–º–æ—Ä–∞–∑—Ä—ã–≤' in text.lower() or 'NORD' in text or 'TUNDRA' in text:
                    door_data['category'] = '–° —Ç–µ—Ä–º–æ—Ä–∞–∑—Ä—ã–≤–æ–º'
                elif 'WHITE' in text or 'VERSAL' in text or '–±–µ–ª' in text.lower():
                    door_data['category'] = '–ë–µ–ª—ã–µ –¥–≤–µ—Ä–∏'
                else:
                    door_data['category'] = '–û—Å–Ω–æ–≤–Ω–æ–π –∫–∞—Ç–∞–ª–æ–≥'
                
                if door_data.get('name'):
                    doors.append(door_data)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                continue
        
        return doors
    
    def parse_door_detail(self, url: str) -> Dict:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –¥–≤–µ—Ä–∏"""
        soup = self.get_page(url)
        if not soup:
            return {}
        
        detail_data = {}
        
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            specs = soup.find_all(['div', 'li', 'tr'], 
                                 class_=re.compile(r'spec|characteristic|param'))
            
            characteristics = {}
            for spec in specs:
                key_elem = spec.find(['span', 'td', 'dt'], 
                                   class_=re.compile(r'key|label|name'))
                val_elem = spec.find(['span', 'td', 'dd'], 
                                   class_=re.compile(r'value|data'))
                
                if key_elem and val_elem:
                    key = key_elem.get_text(strip=True)
                    value = val_elem.get_text(strip=True)
                    characteristics[key] = value
            
            detail_data['characteristics'] = characteristics
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            description = soup.find(['div', 'p'], 
                                  class_=re.compile(r'description|about'))
            if description:
                detail_data['description'] = description.get_text(strip=True)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = soup.find_all('img', class_=re.compile(r'gallery|product'))
            detail_data['images'] = [
                img.get('src') or img.get('data-src') 
                for img in images if img.get('src') or img.get('data-src')
            ]
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–µ—Ç–∞–ª–µ–π: {e}")
        
        return detail_data
    
    def parse_all(self, deep_parse: bool = False):
        """–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞"""
        print("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ –õ–∞–±–∏—Ä–∏–Ω—Ç...")
        print(f"üìç URL: {self.catalog_url}")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
        soup = self.get_page(self.catalog_url)
        if not soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ç–∞–ª–æ–≥")
            return
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        self.doors_data = self.parse_catalog_page(soup)
        
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–≤–µ—Ä–µ–π: {len(self.doors_data)}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π –¥–≤–µ—Ä–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if deep_parse and self.doors_data:
            print("\nüîé –ù–∞—á–∏–Ω–∞—é –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–π –¥–≤–µ—Ä–∏...")
            for i, door in enumerate(tqdm(self.doors_data, desc="–î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥")):
                if door.get('url'):
                    details = self.parse_door_detail(door['url'])
                    self.doors_data[i].update(details)
                    time.sleep(1)  # Respect rate limiting
    
    def save_to_json(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        if not filename:
            filename = f"labirint_catalog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.doors_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ JSON: {filename}")
        return filename
    
    def save_to_csv(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV"""
        if not filename:
            filename = f"labirint_catalog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        if not self.doors_data:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        df = pd.DataFrame(self.doors_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ CSV: {filename}")
        return filename
    
    def save_to_excel(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Excel"""
        if not filename:
            filename = f"labirint_catalog_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        if not self.doors_data:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        df = pd.DataFrame(self.doors_data)
        df.to_excel(filename, index=False, engine='openpyxl')
        
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Excel: {filename}")
        return filename
    
    def print_summary(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if not self.doors_data:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
            return
        
        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
        print("="*60)
        print(f"üì¶ –í—Å–µ–≥–æ –¥–≤–µ—Ä–µ–π: {len(self.doors_data)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–Ω–∞–º
        prices = [d.get('price') for d in self.doors_data if d.get('price')]
        if prices:
            print(f"üí∞ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {min(prices):,} —Ä—É–±.")
            print(f"üí∞ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞: {max(prices):,} —Ä—É–±.")
            print(f"üí∞ –°—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞: {sum(prices)//len(prices):,} —Ä—É–±.")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = {}
        for door in self.doors_data:
            cat = door.get('category', '–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"\nüìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"   {cat}: {count}")
        
        print("="*60 + "\n")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     –ü–ê–†–°–ï–† –ö–ê–¢–ê–õ–û–ì–ê –î–í–ï–†–ï–ô –õ–ê–ë–ò–†–ò–ù–¢                    ‚ïë
    ‚ïë     https://labirintdoors.ru/katalog2                  ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    parser = LabirintParser()
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞
    parser.parse_all(deep_parse=False)  # deep_parse=True –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    
    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    parser.print_summary()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if parser.doors_data:
        parser.save_to_json()
        parser.save_to_csv()
        parser.save_to_excel()
        
        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")


if __name__ == "__main__":
    main()
